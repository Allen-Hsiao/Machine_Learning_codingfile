# -*- coding: utf-8 -*-
"""hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jayy4GswSpaATyzzq7XMS4C2-pAoL8oZ

# CS171-EE142 - Fall 2021 - Homework 4

# Due: Monday, December 3, 2021 @ 11:59pm 

### Maximum points: 45 pts


## Submit your solution to Gradescope:
1. Submit screenshot images or a single PDF to **HW4**
2. Submit your jupyter notebook to **HW4-code**

### Enter your information below:

<div style="color: #000000;background-color: #EEEEFF">
    Your Name (submitter): Hsiangwei Hsiao <br>
    Your student ID (submitter): 862254811
    
<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>
</div>

## Academic Integrity
Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.

# Overview 
In this assignment we will implement and test K-means algorithm for clustering and principal component analysis (PCA) for dimensionality reduction.  

If you are asked to **implement** a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, please ask. 

Before you start, make sure you have installed all those packages in your local Jupyter instance

## Read *all* cells carefully and answer all parts (both text and missing code)

You will complete all the code marked `TODO` and answer descriptive/derivation questions
"""

import numpy as np
import matplotlib.pyplot as plt
import random

# make sure you import here everything else you may need

"""## Question 1. K-Means Clustering [Total: 30 pts]

In this exercise we will first implement K-means algorithm for clustering. Then we will perform color-based segmentation using K-means.

## K-means clustering implementation [10 pts]

Let us first implement K-means algorithm that accepts target number of clusters (K) and data vectors $\mathbf{x}_1,\ldots, \mathbf{x}_N$, each of length $d$. At this point, we will implement the K-means algorithm for general $d$; later we will test and visualize the results for $d=2,3$. 
			
A general K-means algorithm can be described as follows. 
Suppose we are given training examples $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N$, where each $\mathbf{x}_i \in \mathbb{R}^d$. We want to group the $N$ data samples into K clusters.  
* Initialize cluster centers $\mu_1,\ldots, \mu_K \in \mathbb{R}^d$ at random
* Repeat until convergence 
  * For every data point $\mathbf{x}_i$, update its label as 
  \begin{equation*}
  z_i = \text{argmin}_j \|\mathbf{x}_i - \mu_j\|_2^2. 
  \end{equation*}
  
  * For each cluster $j$, update its center $\mu_j$ as mean of all points assigned to cluster $j$: 
  $$ \mu_j = \dfrac{\sum_{i=1}^N \delta\{z_i = j\} \mathbf{x}_i}{\sum_{i=1}^N \delta \{z_i = j\}}.$$  
  $\delta\{z_i = j\}$ denotes an indicator function that is equal to 1 if $z_i = j$ and zero otherwise. $\sum_{i=1}^N\delta\{z_i = j\}$ indicates the number of points in $i$th cluster. 

We can define sum of squared errors (SSE) as 
$$ \text{SSE}  = \sum_j \sum_i \delta\{z_i = j\} \|\mathbf{x}_i - \mu_j\|_2^2$$`

Implement the K-means clustering algorithm as a function with the following specifications:

```python
def kmeans_clustering(data, K, max_iter = 100, tol = pow(10,-3)):
```
where 
1. 'data' is the $N \times d$ matrix that contains all data points ($N$ is the number of data points and $d$ is the number of features, each row of the matrix is a data point), 
2. 'K' is the number of clusters, 
3. 'max_iter' is the maxium number of iterations, and 
4. 'tol' is the tolerance for the change of the sum of squares of errors that determines convergence.

Your function should return the following variables: 
1. 'labels': this is an $N\times 1$ vector (where $N$ is the number of data points) where the $i$-th position of that vector contains the cluster number that the $i$-th data point is assigned to, 
2. 'centroids': this is a $K\times d$ matrix, each row of which contains the centroid for every cluster, 
3.  'SSE_history': this is a vector that contains all the sum of squares of errors per iteration of the algorithm, 
4. 'iters': this is the number of iterations that the algorithm ran.

Here we are going to implement the simplest version of K-means, where the initial centroids are chosen entirely at random among all the data points.

Your algorithm should converge if 1) the maximum number of iterations is reached, or 2) if the SSE between two consecutive iterations does not change a lot. 

In order to check for the latter condition, you may use the following piece of code:
```python
if np.absolute(SSE_history[it] - SSE_history[it-1])/SSE_history[it-1] <= tol
```
"""

k = 3
a = np.array([[2,3],[4,1],[0,5],[4,4]])
b = np.array([[2,2],[3,3],[4,4]])
z = []
s = []
for i in a:
  for y in b:
    z.append(pow(np.linalg.norm(i-y),2))
  s.append(np.argmin(z))
  z = []
print(pow(np.linalg.norm(a[1]-b[1]),2))
print(s)
s = np.array(s)
A = s == 0
print(a[A])

result = 0
for i in range(k):
  for x in a[s==i]:
    result += pow(np.linalg.norm(x-b[i]),2)
print(result)

k = 3
for i in range(k):
  if (i in s):
    b[i] = sum(a[i == s])/a[i==s].shape[0]
    # print(sum(a[i == s])/a[i==s].shape[0])
print(b)

# TODO 
# K-means clustering
def kmeans_clustering(data, K, max_iter=100, tol = pow(10,-3)): 
    #initialize cluster center
    centroids = []
    SSE_history = []
    for i in range(K):
      centroids.append(data[random.randint(0,data.shape[0]-1)])
    centroids = np.array(centroids)
    #update the label to each data point
    z = [] #label
    # z = np.array(z)
    temp_dis = []
    iter = 0
    for iter_time in range(max_iter):
      z = []
      for x in data:
        for y in centroids:
          temp_dis.append(pow(np.linalg.norm(x-y),2))
        z.append(np.argmin(temp_dis))
        temp_dis = []  
      z_arry = np.array(z)
      for i in range(K):
        if ((i in z_arry) and (data[i == z_arry].shape[0] != 0)):
          centroids[i] = sum(data[i == z_arry])/data[i == z_arry].shape[0]
      #compute SSE
      SSE = 0
      for i in range(K):
        for x in data[z_arry==i]:
          SSE += pow(np.linalg.norm(x-centroids[i]),2)
      
      labels = z_arry.transpose()
      # centroids = np.array(centroids)
      # centroids = centroids.transpose()
      SSE_history.append(SSE) #record SSE history
      iter += 1 #count iteration
      #check if it should be end of the iteration
      if iter_time > 0:
        if np.absolute(SSE_history[iter_time] - SSE_history[iter_time-1])/SSE_history[iter_time-1] <= tol:
          break
    
    return labels, centroids, SSE_history, iter

"""### Test K-means on simulated data with different values of K [3 pts]

Let us create synthetic data with `num_clusters` clusters in 2-dimensional space and apply K-means clustering. 

You should try with different values of `num_clusters` and `K'. 


"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from sklearn.datasets import make_blobs

num_clusters = 4
X, y_true = make_blobs(n_samples=400,centers=num_clusters,cluster_std=3,random_state=10)

plt.scatter(X[:, 0], X[:, 1], c=y_true, s=40, cmap='viridis')
ax = plt.gca()
ax.axis('equal')

"""Let us now run K-means using different values of K.

Try `K = [num_clusters-1, num_clusters, num_clusters+1]`
"""

# TODO 
# Run K-means for different values of K. 

for K in [num_clusters-1,num_clusters,num_clusters+1]:
  labels, centroids, SSE_history, iters = kmeans_clustering(X,K)

  # TODO 
  # plot your data (one color per cluster)
  print("K = ", K)
  plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
  ax = plt.gca()
  ax.axis('equal')
  # TODO 
  # plot SSE over different iterations 
  plt.figure()
  plt.plot(SSE_history)
  plt.xlabel("Iterations")
  plt.ylabel("SSE")
  plt.show()

"""### Answer the following questions [4 pts]

- Q: What happens to SSE at every iteration of K-means algorithm (did it increase/decrease)? [1 pt]

> A: SSE is getting decrease until the SSE is converge.


- Q: How does the SSE change as you increase K ? [1 pt]

> A:  SSE will converge to smaller value when the K is getting biger.

- Q: What happens if you increase the `cluster_std` of blobs above to `cluster_std=5` ? [2 pts] 
```
X, y_true = make_blobs(n_samples=400,centers=num_clusters,cluster_std=3,random_state=10)
```

> A: There are more data points overlap in each cluster. The cluster model is worser than the original mode.

### Color segmentation/quantization [10 pts]

Now we will use K-means to perform segmentation/quantization on a color image. 

Each pixel in a given image will be a feature vector containing 3 color components (RGB). We will first group all colors into K clusters, which will provide us a color palatte. Then we will replace the color in every pixel with one of the colors in the color palatte (that is the centroid of the cluster in which RGB value of a pixel falls). 

We will use K-means script from previous step to segment your image into K clusters. To create a "quantized" output image, replace every pixel in your image with the center of the cluster assigned to it. Report your results for K= $\{2,4, 8, 16, 32, 64\}$ clusters.

__You will need a colorful selfie__

Take a _selfie_ of yourself with a background that has different colors from your skin and clothing. 

Let us say you name the image `selfie.jpg`
"""

from google.colab import drive
drive.mount('/content/drive')

# load and display an image with Matplotlib
from matplotlib import image

# load image as pixel array
img = image.imread('/content/drive/MyDrive/EE_142_Machine_Learning/selfie.jpg')
# summarize shape of the pixel array
print(img.dtype)
print(img.shape)

# You can debug your code using some other image, 
# but you must use your selfie for final results
# from sklearn.datasets import load_sample_image
# # load the picture 
# img = load_sample_image('china.jpg')

h, w, c = img.shape
unique_colors = np.unique(np.reshape(img,(h*w,c)),axis = 0)

plt.figure()
plt.clf()
ax = plt.axes([0, 0, 1, 1])
# plt.axis('off')
plt.grid(False)
plt.title('Original image with {0:d} unique colors.'.format(unique_colors.shape[0]))
plt.imshow(img)
plt.show()

"""Before performing the clustering, we will process the image data. 

1. You can crop or resize your image to a small size if the image is large. An image close to $100\times 100$ pixels will be sufficient for this experiment. 

2. Convert 8 bit integers to floats by dividing every pixel by 255 so that we can perform floating point operations and plot the arrays as images using plt.imshow that works well on float data in the range [0-1].


"""

# Resize image to speed things up

# Improting Image class from PIL module 
from PIL import Image 

# use resize function if needed 

# TODO (if necessary)

img = Image.open('/content/drive/MyDrive/EE_142_Machine_Learning/selfie.jpg')
img_size = img.size

# modify the size 
new_size = np.asarray(img_size)/30
img = img.resize(new_size.astype(int))
img = np.asarray(img)
plt.imshow(img)
plt.grid(False)

# Preprocessing the data for clustering
# convert to float64 in range [0,1]
if np.max(img) > 1:
  img = np.array(img, dtype=np.float64) / 255
plt.imshow(img)
plt.grid(False)

# Load Image and transform to a 2D numpy array.
h, w, c = original_shape = tuple(img.shape)
assert c == 3
data = np.reshape(img, (w * h, c))

plt.figure()
plt.scatter(data[:,0],data[:,1],data[:,2])

"""Now we will perform two steps. 

1. K-means clustering for different values of K using the `kmeans_clustering` function above. 

2. Replace all the colors in the original image with the centroids of the cluster each of them belong to. This will give us a "segmented/quantized image" for different values of K; let us denote each image as `img_seg`. 

Plot the original image and the `img_seg` for `K = 2, 4, 8, 16, 32, 64.` 
"""

# TODO
# For K = 2, 4, 8, 16, 32, 64 
  # Perform K-means clustering for different values of K on RGB pixels; 
  # this will give you K RGB values as centroids of K clusters

  # Create a quantized image based on your cluster assignment
  # Plot original and quantized images 
print("Original image")
plt.imshow(img)
plt.grid(False)
plt.show()
for K in [2, 4, 8]:
  labels, centroids, SSE_history, iters = kmeans_clustering(data,K,max_iter=100,tol=pow(10,-3))  
  img_seg = centroids[labels]
  img_seg = img_seg.reshape(img.shape)
  print("K = ", K)
  plt.imshow(img_seg)
  plt.grid(False)
  plt.show()

print("Original image")
plt.imshow(img)
plt.grid(False)
plt.show()
for K in [16, 32, 64]:
  labels, centroids, SSE_history, iters = kmeans_clustering(data,K,max_iter=100,tol=pow(10,-3))  
  img_seg = centroids[labels]
  img_seg = img_seg.reshape(img.shape)
  print("K = ", K)
  plt.imshow(img_seg)
  plt.grid(False)
  plt.show()

"""### Answer the following questions [3 pts]

- Q: How many unique colors you have in the quantized image for K=2? [1 pt]

> A: There are two color in the quantized image.

- Q: How is the quality of "quantized image" affected as you increase K? [1 pt]

> A: Increasing K can have image more colors and the shape can also be more clear.

- Q: What value of K provides you best quality for the "quantized image"? [1 pt]

> A: When K = 64, the quantized image is more closer than original image.

## Question 2. Eigen Faces via Principal Component Analysis [15 pts]

In this question, we will 

- Compute the PCA for a simple data set using SVD.
- Visualize the PCA for images

__Load dataset__

We will use a dataset of cropped face images called LFW ("Labeled Faces in the Wild").

This face dataset was taken from news articles about 10 years ago.  The full data set has thousands of faces, but we will use a small subset here.  Since this data set is widely-used, it is installed as part of the `sklearn`. 

We first download the data.  This is large and can take several minutes.
"""

from sklearn.datasets import fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

import numpy as np
import matplotlib.pyplot as plt
import random

"""Once the data is loaded, we can get see the dimensions"""

# Get images
n_samples, h, w = lfw_people.images.shape
npix = h*w

# Data in 2D form
X = np.transpose(lfw_people.data) 

# NOTE: In many of the libraries the data is stored as N x d array, 
# where N is the number of training samples and d is the length of each data vector
# Since we use a different notation in the class, I will stick with that. 

# Data array has dimensions 1850 x 1288 -- (pixels) x (faces)
# Each data vector is stored as a column in the matrix. 

# Labels of images 
y = lfw_people.target
target_names = lfw_people.target_names 

print("Image size     = {0:d} x {1:d} = {2:d} pixels".format(h,w,npix))
print("Number faces   = {0:d}".format(n_samples))

"""__Plotting the Faces__

We will plot a few faces to look at the dataset.
"""

def plt_face(x):
    h = 50
    w = 37
    plt.imshow(x.reshape((h, w)), cmap=plt.cm.gray)
    plt.xticks([])
    plt.yticks([])
    
I = np.random.permutation(n_samples)
plt.figure(figsize=(10,20))

T = 4
for i in range(T):    
    ind = I[i]
    plt.subplot(1,T,i+1)
    plt_face(X[:,ind])
    plt.title(target_names[y[ind]])

"""### Computing the PCA via SVD 
To compute principal components (PCs), you will perform two steps. 

1. Subtract the mean from the data set.
2. Compute the singular value decomposition (SVD)

Suppose $X$ is the $D\times N$ data matrix. ($D$ is the number of pixels in each image, $N$ is the number of images.)

You will first remove mean column from all the columns (because data vectors are stored as columns). 

``` Xs = X - np.mean(X,1,keepdims = True) ``` 

Then we will compute an SVD of mean-substracted data as 

``` U_, S_, Vt_ = np.linalg.svd(Xs, full_matrices=False)```

Note that in python the SVD returns a list of singular values and `V.T` instead of `V`. 
The `full_matrices` option gives the *economy* SVD

"""

# Compute SVD 

# subtract mean from the dataset 
# since data vectors are stored as columns, the mean of columns should be zero

npix = h*w
Xmean = np.mean(X,1,keepdims=True)
Xs = X - Xmean

U_,S_,Vt_ = np.linalg.svd(Xs, full_matrices=False)

# We can plot the singular values

plt.figure()
plt.plot(S_)
plt.ylabel('singular values')
plt.xlabel('index')

"""Next we will approximate the data using $r$ principal components/factors as

    X ~= Xhat = Xmean + U.dot(Z) 
    
The terms

*  `U` are the top-$r$ principal components/factors from `U_` computed from data.
*  `Z` are the coefficients of data samples (or projections onto principal components). Note that we can compute $Z = U^T X$.  

For instance, we can compute $Z$ for $r = 20$ and the approximate data as
"""

# Compute Z for r = 20 
N = X.shape[1]
Z = U_.T.dot(Xs) 

r = 20
Xest = Xmean + U_[:,:r].dot(Z[:r,:])

# plot some sample images
T = 3; 
inds = np.random.permutation(n_samples)
inds = inds[:T]
print(inds)

# plot approximated faces
print('Approximated faces (above)')
for i in range(T):
  plt.subplot(1,T,i+1)
  # plt.imshow(np.reshape(Xest[:,inds[i]],(h,w)))
  plt_face(Xest[:,inds[i]])
  plt.grid(False)
  plt.axis('off')
  plt.title(target_names[y[inds[i]]])
  if i == 0:
    plt.ylabel('Approximation')

plt.show()

# plot original faces 
for i in range(T):
  plt.subplot(1,T,i+1)
  # plt.imshow(np.reshape(X[:,inds[i]],(h,w)))
  plt_face(X[:,inds[i]])
  plt.grid(False)
  plt.axis('off')
  # plt.title(target_names[y[inds[i]]])
  if i == 0:  
    plt.ylabel('Original')

plt.show()
plt.tight_layout()

print('Original faces (below)')

"""### Plotting Approximated Images from PCs [10 pts]

We will now plot the images approximated using different values of $r$. 

Select 3 images at random from the dataset and find its approximation using top  $r$ factors for $r = \{0, 5, 10, 50, 100, 200\}$. 
"""

# TODO 
  # select 3 images
  # select r = [0, 5, 10, 50, 100, 200]
  # find approximation of selected images and plot them

T = 3                             # number of faces to plot
r_list = [0, 5,10,50,100, 200]    # number of SVD approximations
nr = len(r_list)

# TODO 
# Select random faces 
inds = np.random.permutation(n_samples)
inds = inds[:T]

#Compute SVD
npix = h*w
Xmean = np.mean(X,1,keepdims=True)
Xs = X - Xmean

U_,S_,Vt_ = np.linalg.svd(Xs, full_matrices=False)

# TODO 
# Loop over figures
# compute approximation with r factors 
iplt = 0
for ind in inds:
    for r in r_list:
        plt.subplot(T,nr+1,iplt+1)
        
        # Reconstruct with SVD
        # TODO  
        # Xhati = ???  # estimate of image in the r-dim subspace
        N = X.shape[1]
        Z = U_.T.dot(Xs)
        Xhati = Xmean + U_[:,:r].dot(Z[:r,:])
        Xhati = Xhati[:,inds[i]]
        plt_face(Xhati)
        plt.title('r={0:d}'.format(r))
        iplt += 1
    
    # Plot the true face
    plt.subplot(T,nr+1,iplt+1)
    plt_face(X[:,ind])
    plt.axis('off')
    plt.title('Full')
    iplt += 1

"""Finally, pick 10 top PCs and plot them as images. """

# TODO 
  # Plot first 10 PCs (i.e., columns in U_) as images

npix = h*w
Xmean = np.mean(X,1,keepdims=True)
Xs = X - Xmean

U_,S_,Vt_ = np.linalg.svd(Xs, full_matrices=False)


for i in range(10):
  plt.subplot(1,10,i+1)
  plt_face(U_[:,i])
  plt.grid(False)
  plt.axis('off')

"""### Best and Worst Approximation [5 pts]

You can convince yourself that the $\ell_2$ norm of the approximation error of any data vector is same as the $\ell_2$ norm of the unused coefficients. 

Using this property, find the best and worst 5 images in the dataset that give smallest and largest error in approximation with top 200 PCs? Plot them. 
"""

# We can compute the L2 norm of the approximation error for each image as 
# the L2 norm of the unused coefficients

r = 200
Xest = Xmean + U_[:,:r].dot(Z[:r,:])

est_error = np.sum(pow(Xest-X,2),axis=0)
plt.figure()
plt.plot(est_error)
plt.xlabel('image index')
plt.ylabel('approximation error')


Z_residual = np.sum(pow(Z[r:,:],2),axis=0)
plt.figure()
plt.plot(Z_residual)
plt.xlabel('image index')
plt.ylabel('Z residual')

# TODO
# Plot the "Worst/Hardest" images to reconstruct with 200 PCs

# Sort error/residual in decreasing order and pick top 5 indices 

# your code goes here  
Max_5_error = np.argsort(est_error)[-5:]
i = 0
for x in Max_5_error:
  plt.subplot(1,5,i+1)
  plt_face(Xest[:,x])
  plt.grid(False)
  plt.axis('off')
  i += 1

er = est_error/Z_residual
er_top5 = np.argpartition(er, 5)[:5]
print("Top 5 indices of error/residual: ", er_top5)

# TODO
# Plot the "Best/Easiest" images to reconstruct with 200 PCs

# Sort error/residual in increasing order and pick top 5 indices 

# your code goes here 
Min_5_error = np.argpartition(est_error, 5)[:5]
i = 0
for x in Min_5_error:
  plt.subplot(1,5,i+1)
  plt_face(Xest[:,x])
  plt.grid(False)
  plt.axis('off')
  i += 1

er = est_error/Z_residual
er_top5 = np.argpartition(er, -5)[-5:]
print("Top 5 indices of error/residual: ", er_top5)