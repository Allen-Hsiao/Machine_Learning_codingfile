# -*- coding: utf-8 -*-

"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qy0NUGjLkpLrtdAreT_b9F6Ws4SlFTm

"""
import random as rand
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# make sure you import here everything else you may need

"""# Question 0: Getting real data [0 pts]
In this assignment you are going to use data from the UCI Machine Learning repository ( https://archive.ics.uci.edu/ml/index.php ). In particular, you are going to use the famous Iris dataset: https://archive.ics.uci.edu/ml/datasets/Iris
"""

data_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label']
data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names = data_names)
data.head()

"""This data has 150 samples. Each sample has 4 features that are given as ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] and one label. 

The label has three possible values ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
"""

data.shape
# data.head(5)
# data.tail(5)

"""*Note that the arrangement of data samples is different from the notation we used in the class. In the class, data samples are given as column vectors. Here data samples are given as row vectors.*

`data` has 150 sample vectors, each of length 4 and stored as rows of `data`. 
"""

data.label.unique()

"""We can visualize pair-wise relations among 4 features in the data using scatterplot of all pairs of features and color the points by class label """

sb.pairplot(data, hue = 'label')

"""## Question 1: Basic data analysis [25 pts]

## Question 1a: Counting and simple statistics [5]

1. Calculate and print the mean value of each of the four features in the entire dataset [1]

1. Count and print the number of samples that belong to each of the three categories [1] 

1. Calculate and print the mean of each feature for each each label as a $3\times 4$ table. [3]
"""

# TODO 
# your code here 
data.describe()
# 1. print mean value of 4 features 
print(data.mean(axis=0))
print('------------------------')
# 2. print number of samples per category
print(data['label'].value_counts())
print('------------------------')
# 3. print 3x4 table of mean value of feature for each label 
print(data.groupby('label').mean())
print('------------------------')
matrix_data = data.drop(['label'], axis=1)
# matrix_data.values

"""### Question 1b: Implement the $\ell_p$ distance function [10]
1. Write code that implements the Lp distance function between two data points as we saw it in class [7] \\
$\ell_p$ distance between two vectors $\mathbf{x}_i, \mathbf{x}_j$ in $\mathbb{R}^d$ can be written as  
$$\|\mathbf{x}_i - \mathbf{x}_j\|_p = \left(\sum_{k=1}^d |\mathbf{x}_i(k) - \mathbf{x}_j(k)|^p\right)^{1/p}. $$
You should implement this function yourself. Do not use any built-in function to compute distance or norm. 

2. Verify that it is correct by comparing it for p=1 and p=2 against an existing implementation in Numpy for the two selected data points below. Note that the difference of the distances may not be exactly 0 due to numerical precision issues. [3]
"""

# TODO
# your code here
import math
# def distance(x,y,p=2):
#   # implement Lp distance function 
def distance(x, y, p=2):
  sum_points = (x-y)**p
  sum_points = abs(sum_points)
  result = pow(sum_points,1/p)
  return result

def distance_more_dim(x, y, p=2):
  sum_points = 0.0
  for i in range(len(x)):
    sum_points += (abs(x[i] - y[i])**p)
  result = pow(sum_points,1/p)
  return result
print(distance_more_dim([1,1],[2,2], p=2))

def distance_more_dim(x, y, p=2):
  sum_points = 0.0
  for i in range(len(x)):
    sum_points += (abs(x[i] - y[i])**p)
  result = pow(sum_points,1/p)
  return result
print(distance_more_dim([5.1, 3.5, 1.4, 0.2], [5.4, 3.7, 1.5, 0.2], p=1)) #data point0 and data point10
print(distance_more_dim([5.1, 3.5, 1.4, 0.2], [5.4, 3.7, 1.5, 0.2], p=2)) #data point0 and data point10

"""### Question 1c: Compute the distance matrix between all data points [10]
1. Compute an $N\times N$ distance matrix between all data points (where $N=150$ is the number of data points) [3]
2. Plot the above matrix and include a colorbar. [3]
3. What is the minimum number of distance computations that you can do in order to populate every value of this matrix? (note: it is OK if in the first two questions you do all the $N^2$ computations) [2]
4. Note that the data points in your dataset are sorted by class. What do you observe in the distance matrix? [2]
"""

# TODO 
# your code here
import pandas as pd
import numpy as np
def dist_matrix(data,p=2):
  df = pd.DataFrame(data)
  dis_matrix = np.zeros(((df.values).shape[0],(df.values).shape[0]))
  n_df = df.values
  for i in range((df.values).shape[0]):
    for j in range((df.values).shape[0]):
      dis_matrix[i, j] = distance_more_dim(n_df[i], n_df[j],p=2)
  return (dis_matrix)
#  # create distance matrix
matrix_data = data.drop(['label'], axis=1)
matrix_data.values
matrix_plot = dist_matrix(matrix_data)
print(matrix_plot)

import matplotlib 
import matplotlib.pyplot as plt

plt.imshow(matrix_plot)
plt.colorbar()
plt.show()

"""Your answer here:
3. The least computations are (150*150-150)/2. The values in distance matrix is symmetry.
4. In the distance matrix, we can observe the relation between each class. The value of distance relates to how closed each class are.

## Question 2: K-Nearest Neighbors Classifier [50 pts]
The K-Nearest Neighbors Classifier is one of the most popular instance-based (and in general) classification models. In this question, we will implement our own version and test in different scenarios.

### Question 2a: Implement the K-NN Classifier [30]
For the implementation, your function should have the format:
```python
def knnclassify(test_data,training_data, training_labels, K=1):
```
where 'test_data' contains test data points, 'training_data' contains training data points, 'training_labels' holds the training labels, and 'K' is the number of neighbors. 

The output of this function should be 'pred_labels' which contains the predicted label for each test data point (it should, therefore, have the same number of rows as 'test_data').

The piece of code below prepares the Iris dataset by converting the labels from strings to integers (which is quite easier to move around and do calculations with):
"""

all_vals = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
all_labels = data['label'].values
unique_labels = np.unique(all_labels)
#change string labels to numbers
new_labels = np.zeros(len(all_labels))
for i in range(0,len(unique_labels)):
    new_labels[all_labels == unique_labels[i]] = i
all_labels = new_labels

# all_labels
all_vals

# TODO 
# your code here
import math 
#Calculate the euclidean distance two vector points
def distance_more_dim(x, y, p=2):
  sum_points = 0.0
  for i in range(len(x)):
    sum_points += (abs(x[i] - y[i]))**p
  result = pow(sum_points,1/p)
  return result

#Get neighbors points
def get_neighbors(test_data, training_data, training_labels, K=1):
  distance = list()
  neighbors_point = list()
  index = 0
  #Compute distance between training points and test points
  for trainpoints in training_data:
    distance_x = distance_more_dim(test_data, trainpoints)
    label = training_labels[index]
    distance.append((trainpoints, distance_x, label))
    index += 1
    #Sorting the distance from closed to far
  distance.sort(key = lambda dist: dist[1])
  for i in range(K):
    neighbors_point.append(distance[i])

  return neighbors_point

def knnclassify(test_data,training_data, training_labels, K=1):
  pred = list()
  for testpoint in test_data:
    neighbor_points = get_neighbors(testpoint, training_data, training_labels, K)
    output = [row[-1] for row in neighbor_points]
    pred.append(max(set(output), key=output.count))
  return pred

print(knnclassify([[6.1, 3. , 4.9, 1.8]], all_vals, all_labels, 1))

"""### Question 2b: Measuring performance [20]

In this question you will have to evaluate the average performance of your classifier for different values of $K$. In particular, $K$ will range in $\{1,\cdots,10\}$. We are going to measure the performance using classification accuracy. For computing the accuracy, you may use
```python
accuracy = sum(test_labels == pred_labels)/len(test_labels)
```
where 'test_labels' are the actual class labels and 'pred_labels' are the predicted labels


In order to get a proper estimate for the accuracy for every K, we need to run multiple iterations where for each iteration we get a different randomized split of our data into train and test. In this question, we are going to run 100 iterations for every K, and for every random splitting, you may use:

```python
    (training_data, test_data, training_labels, test_labels) = train_test_split(all_vals, all_labels, test_size=0.3)
```
where the train/test ratio is 70/30. 

After computing the accuracy for every $K$ for every iteration, you will have 100 accuracies per $K$. The best way to store those accuracies is in a matrix that has as many rows as values for $K$ and 100 columns, each one for each iteration.

Compute average accuracy as a function of $K$. Because we have a randomized process, we also need to compute how certain/uncertain our estimation for the accuracy per $K$ is. For that reason, we also need to compute the standard deviation of the accuracy for every $K$. Having computed both average accuracy and standard deviation, make a figure that shows the average accuracy as a function of $K$ with each point of the figure being surrounded by an error-bar encoding the standard deviation. You may find 
```python
plt.errorbar()
```
useful for this plot.
"""

# TODO 
# your code here
K_matrix = np.zeros((10,100))
for k in range(10):
  for i in range(100):
    (training_data, test_data, training_labels, test_labels) = train_test_split(all_vals, all_labels, test_size=0.3)
    pred_labels = knnclassify(test_data, training_data, training_labels, k+1)
    accuracy = sum(test_labels == pred_labels)/len(test_labels)
    K_matrix[k][i] = accuracy

K_matrix.shape

import matplotlib.pyplot as plt

Average_Accuracy = K_matrix.mean()
Standard_deviation = K_matrix.std(axis=1)

x = [1,2,3,4,5,6,7,8,9,10]
y = K_matrix.mean(axis=1)
print(Standard_deviation)

plt.scatter(x, y)
plt.xlim([0, 11])
plt.ylim([0.9, 1])
plt.errorbar(x, y, yerr=Standard_deviation)
plt.show()
